{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to implment Tiny Darknet model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n",
    "    '''Preprocess true boxes to training input format\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: array, shape=(m, T, 5)\n",
    "        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n",
    "    input_shape: array-like, hw, multiples of 32\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n",
    "    '''\n",
    "    assert (true_boxes[..., 4]<num_classes).all(), 'class id must be less than num_classes'\n",
    "    num_layers = len(anchors)//3 # default setting\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]]\n",
    "\n",
    "    true_boxes = np.array(true_boxes, dtype='float16')\n",
    "    input_shape = np.array(input_shape, dtype='int16')\n",
    "    boxes_xy = true_boxes[..., 0:2]\n",
    "    boxes_wh = true_boxes[..., 2:4]\n",
    "\n",
    "    m = true_boxes.shape[0]\n",
    "    grid_shapes = [input_shape//{0:32, 1:16, 2:8}[l] for l in range(num_layers)]\n",
    "    y_true = [np.zeros((m,grid_shapes[l][0],grid_shapes[l][1],len(anchor_mask[l]),5+num_classes),\n",
    "        dtype='float32') for l in range(num_layers)]\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "    anchor_maxes = anchors / 2.\n",
    "    anchor_mins = -anchor_maxes\n",
    "    valid_mask = boxes_wh[..., 0]>0\n",
    "\n",
    "    for b in range(m):\n",
    "        # Discard zero rows.\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh)==0: continue\n",
    "        # Expand dim to apply broadcasting.\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        box_maxes = wh / 2.\n",
    "        box_mins = -box_maxes\n",
    "\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        box_area = wh[..., 0] * wh[..., 1]\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "\n",
    "        # Find best anchor for each true box\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "\n",
    "        for t, n in enumerate(best_anchor):\n",
    "            for l in range(num_layers):\n",
    "                if n in anchor_mask[l]:\n",
    "                    i = np.floor(true_boxes[b,t,0]*grid_shapes[l][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b,t,1]*grid_shapes[l][0]).astype('int32')\n",
    "                    k = anchor_mask[l].index(n)\n",
    "                    c = true_boxes[b,t, 4].astype('int32')\n",
    "                    y_true[l][b, j, i, k, 0:4] = true_boxes[b,t, 0:4]\n",
    "                    y_true[l][b, j, i, k, 4] = 1\n",
    "                    y_true[l][b, j, i, k, 5+c] = 1\n",
    "\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def box_iou(b1, b2):\n",
    "    '''Return iou tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "    '''\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh/2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxes = b1_xy + b1_wh_half\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh/2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "\n",
    "    intersect_mins = K.maximum(b1_mins, b2_mins)\n",
    "    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss) #return a model with the y_preds and y_true with a specific loss\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(path ,train_txt, name):\n",
    "    list_images = []\n",
    "    list_labels = []\n",
    "    list_anchors = []\n",
    "    list_class  = []\n",
    "\n",
    "    for file in tqdm(open(path+train_txt).readlines()):\n",
    "        file = file.replace('\\n','')\n",
    "        file = file.replace('\\r','')\n",
    "        img_path = path+file[2:]\n",
    "        img_path_copy = img_path\n",
    "    \n",
    "        #target_image = cv2.imread(img_path)\n",
    "        img_path=img_path.replace(\".jpg\",\".txt\")\n",
    "        img_path= img_path.replace(\".png\",\".txt\")\n",
    "        lines = open(img_path).readlines()\n",
    "        anchors =  0\n",
    "        string_label =''\n",
    "        for line in lines:\n",
    "            anchors = anchors+1\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            string_label = string_label+\"!\"+line  # all labels are seperated by !\n",
    "            \n",
    "        if anchors!=0:\n",
    "            list_labels.append(string_label)\n",
    "            list_anchors.append(anchors)\n",
    "            list_images.append(img_path_copy)  #append the path of the image\n",
    "            list_class.append(\"Face\")\n",
    "\n",
    "\n",
    "    images = np.array(list_images)\n",
    "    labels = np.array(list_labels)\n",
    "    anchors = np.array(list_anchors)\n",
    "    classes = np.array(list_class)\n",
    "    training  = pd.DataFrame({'target image path':images , 'labels': labels , 'class': classes , 'anchors': anchors})\n",
    "    training.to_csv(name , index = False)   #write the results to an csv file   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image_from_csv Generator\n",
    "import sklearn\n",
    "def csv_image_generator(csv_file , batch , ls , num_classes, anchors , mode = 'train' , data_augmentation = None): #these are the default values\n",
    "    csv_file = pd.read_csv(csv_file)\n",
    "    csv_file = sklearn.utils.shuffle(csv_file)\n",
    "    csv_file.reset_index(inplace=True, drop=True)\n",
    "    max_boxes = 20\n",
    "    counter = 1\n",
    "    max_rows = len(csv_file.index)\n",
    "    while True: #a generator loops infinitely,hencewe need to use steps per epoch to determine when a batch starts n stop\n",
    "        # initialize our batches of images and labels\n",
    "        input_shape = data_augmentation\n",
    "        images = []\n",
    "        batch_boxes = []\n",
    "\n",
    "        while len(images) < batch: #loop until batch size is met\n",
    "            box_array = np.zeros((max_boxes,5))\n",
    "            row = csv_file.iloc[counter, 0:4]\n",
    "            image = cv2.imread(row['target image path'])\n",
    "            # if the data augmentation \n",
    "            if data_augmentation is not None: #aug has the (width,height) shape of the image\n",
    "                image = cv2.resize(image,data_augmentation) #resize image according to (width,height) either 416x416 or 224x224\n",
    "                \n",
    "            string_label = row['labels'] #get the whole stringlabel\n",
    "            \n",
    "            if not (string_label==\"\" or string_label==\"nan\"):\n",
    "                boxes = string_label.split('!')\n",
    "                boxes = np.array(boxes)\n",
    "                num_anchors = row['anchors'] #number of bounding boxes per image\n",
    "                empty_list = []\n",
    "                for box in boxes:\n",
    "                    if not box=='':\n",
    "                        labels = box.split(\" \")\n",
    "                        labels = np.array(labels)\n",
    "                        class_= labels[0] #get the class id\n",
    "                        box_data = [labels[1], labels[2], labels[3],labels[4],int(labels[0])] #set class id at position 4\n",
    "                        empty_list.append(box_data)\n",
    "                        #ls.transform(class_) activate when class training starts\n",
    "\n",
    "                all_boxes_per_image = np.array(empty_list)\n",
    "                if len(all_boxes_per_image)>max_boxes: all_boxes_per_image = all_boxes_per_image[:max_boxes]\n",
    "                box_array[:len(all_boxes_per_image)] = all_boxes_per_image\n",
    "\n",
    "                # check to see if the line is empty, indicating we have\n",
    "                # reached the end of the file\n",
    "                if counter == max_rows-1 :\n",
    "                    counter = 1 #go back to the beginning of the file\n",
    "                    #print('Start form the beginning of the dataset')\n",
    "\n",
    "                    if mode == \"eval\": #for evaluation mode break if we reached the end of the file\n",
    "                        break\n",
    "                #update batch list\n",
    "                images.append(image)\n",
    "                #print(box_array.shape)\n",
    "                batch_boxes.append(box_array)\n",
    "                \n",
    "            counter = (counter+1)%max_rows\n",
    "            \n",
    "            \n",
    "        #End of batch size loop\n",
    "        batch_boxes = np.array(batch_boxes)\n",
    "        #print(batch_boxes.shape)\n",
    "        y_true = preprocess_true_boxes(batch_boxes ,input_shape, anchors, num_classes)\n",
    "        # yield the batch to the calling function /the generator returns a batch everytime\n",
    "        yield [np.array(images), *y_true], np.zeros(batch) #keep yielding the function to the generator while its true\n",
    "      \n",
    "    #end of while loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(H,N):\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")  #accuracy only applies when we have different classes\n",
    "    plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy on Dataset\") #consider doing an IoU vs number of epochs for every batch\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(\"plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_image_generator_wrapper(csv_file , batch , ls , num_classes, anchors , mode , data_augmentation):\n",
    "    if batch<=0: return None\n",
    "    return csv_image_generator(csv_file , batch , ls , num_classes, anchors ,mode , data_augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiny_yolo_body(inputs, num_anchors, num_classes):\n",
    "    '''Create Tiny YOLO_v3 model CNN body in keras.'''\n",
    "    x1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(16, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(32, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(64, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(128, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n",
    "    x2 = compose(\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            MaxPooling2D(pool_size=(2,2), strides=(1,1), padding='same'),\n",
    "            DarknetConv2D_BN_Leaky(1024, (3,3)),\n",
    "            DarknetConv2D_BN_Leaky(256, (1,1)))(x1)\n",
    "    y1 = compose(\n",
    "            DarknetConv2D_BN_Leaky(512, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))(x2)\n",
    "\n",
    "    x2 = compose(\n",
    "            DarknetConv2D_BN_Leaky(128, (1,1)),\n",
    "            UpSampling2D(2))(x2)\n",
    "    y2 = compose(\n",
    "            Concatenate(),\n",
    "            DarknetConv2D_BN_Leaky(256, (3,3)),\n",
    "            DarknetConv2D(num_anchors*(num_classes+5), (1,1)))([x2,x1])\n",
    "\n",
    "    return Model(inputs, [y1,y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3] # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "        [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "        [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust preditions to each spatial grid point and anchor size.\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[::-1], K.dtype(feats))\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape/image_shape))\n",
    "    offset = (input_shape-new_shape)/2./input_shape\n",
    "    scale = input_shape/new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes =  K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "        anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6,7,8], [3,4,5], [0,1,2]] if num_layers==3 else [[3,4,5], [1,2,3]] # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "            anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "\n",
    "    return boxes_, scores_, classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|█████████████████████████▋                                                    | 234/711 [00:00<00:00, 2182.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 711/711 [00:00<00:00, 3192.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2134/2134 [00:00<00:00, 5059.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tiny YOLOv3 model with 6 anchors and 1 classes.\n",
      "[INFO] training w/ generator...\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n\t [[loss/add_12/_383]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node conv2d_1/convolution:\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\t\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\n\nInput Source operations connected to node conv2d_1/convolution:\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\t\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\n\nOriginal stack trace for 'conv2d_1/convolution':\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-d917d04e8fdb>\", line 53, in <module>\n    model = create_tiny_model(INPUT_SHAPE, ANCHORS, NUM_CLASSES, load_pretrained= False, freeze_body=2, weights_path='keras-yolo3/model_data/tiny_yolo_weights.h5')\n  File \"<ipython-input-6-fb1c1dbca6cc>\", line 11, in create_tiny_model\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\model.py\", line 100, in tiny_yolo_body\n    DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  [Previous line repeated 7 more times]\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 894, in convolution\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 971, in convolution_internal\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[loss/add_12/_383]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d917d04e8fdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m H = model.fit_generator(train_gen, steps_per_epoch=NUM_TRAIN_IMAGES // BATCH, \n\u001b[0;32m     80\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_gen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_TEST_IMAGES\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m                         epochs=EPOCHS,initial_epoch = 0 ,callbacks = [model_checkpoint])\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n\t [[loss/add_12/_383]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node conv2d_1/convolution:\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\t\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\n\nInput Source operations connected to node conv2d_1/convolution:\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\t\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\n\nOriginal stack trace for 'conv2d_1/convolution':\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-d917d04e8fdb>\", line 53, in <module>\n    model = create_tiny_model(INPUT_SHAPE, ANCHORS, NUM_CLASSES, load_pretrained= False, freeze_body=2, weights_path='keras-yolo3/model_data/tiny_yolo_weights.h5')\n  File \"<ipython-input-6-fb1c1dbca6cc>\", line 11, in create_tiny_model\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\model.py\", line 100, in tiny_yolo_body\n    DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  [Previous line repeated 7 more times]\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 894, in convolution\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 971, in convolution_internal\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "#Input images need to be OF SPECIFIC SIZE 224X224\n",
    "#import all necessary libraries\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from yolo3.model import yolo_body, tiny_yolo_body, yolo_loss\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "\n",
    "\n",
    "#check keras gpu is running\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "sess = tf.Session(config=config) \n",
    "K.set_session(sess)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "\n",
    "train_txt = \"dataset/fddb/FDDB-folds/annotations_darknet/train.txt\"\n",
    "test_txt = \"dataset/fddb/FDDB-folds/annotations_darknet/test.txt\"\n",
    "path = \"D:/Desktop/Facial-Recognition/Face-Detection/\" #path to Face Detection folder\n",
    "\n",
    "make_csv(path,train_txt, \"training.csv\") #this creates a training.csv file with everything needed for training\n",
    "make_csv(path , test_txt, \"testing.csv\")  #this created the testing csv file\n",
    "training_data = pd.read_csv(\"training.csv\")       \n",
    "#image =  cv2.imread(training_data.iat[0,0])\n",
    "#cv2.imshow(\"test\", image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "#print(image.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Set trainig parameters\n",
    "TRAIN_CSV = \"training.csv\"\n",
    "TEST_CSV = \"testing.csv\"\n",
    "INPUT_SHAPE = (416,416)\n",
    "ANCHORS = np.array([10,14,  23,27,  37,58,  81,82,  135,169,  344,319]).reshape(-1, 2) #the anchors go by pairs\n",
    "EPOCHS = 3\n",
    "BATCH = 5\n",
    "NUM_CLASSES = 1\n",
    "\n",
    "\n",
    "\n",
    "model = create_tiny_model(INPUT_SHAPE, ANCHORS, NUM_CLASSES, load_pretrained= False, freeze_body=2, weights_path='keras-yolo3/model_data/tiny_yolo_weights.h5')\n",
    "model_checkpoint = ModelCheckpoint(\"model.h5\", monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "# Count the samples/#of rows for testing and training dataset\n",
    "NUM_TRAIN_IMAGES = (pd.read_csv(TRAIN_CSV)).shape[0]\n",
    "NUM_TEST_IMAGES = (pd.read_csv(TEST_CSV)).shape[0]\n",
    "\n",
    "\n",
    "object_encoder = LabelBinarizer()\n",
    "object_encoder.fit([\"Face\", \"Nada\"])\n",
    "class_encoder = LabelEncoder()\n",
    "class_encoder.fit([\"Face\", \"Nada\"])\n",
    "\n",
    "#testclasses = lb.transform(test_classes)\n",
    "\n",
    "\n",
    "# initialize both the training and testing image generators\n",
    "train_gen = csv_image_generator_wrapper(TRAIN_CSV, BATCH, object_encoder,NUM_CLASSES, ANCHORS, \"train\", INPUT_SHAPE)\n",
    "test_gen = csv_image_generator_wrapper(TEST_CSV, BATCH, object_encoder ,NUM_CLASSES,ANCHORS, \"eval\", INPUT_SHAPE)\n",
    "\n",
    "# initialize our Keras model and compile it\n",
    "\n",
    "opt = SGD(lr=1e-2, momentum=0.9, decay=1e-2 / EPOCHS)\n",
    "model.compile(loss={'yolo_loss': lambda y_true, y_pred: y_pred}, optimizer=opt)\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training w/ generator...\")\n",
    "H = model.fit_generator(train_gen, steps_per_epoch=NUM_TRAIN_IMAGES // BATCH, \n",
    "                        validation_data=test_gen,validation_steps=NUM_TEST_IMAGES // BATCH,\n",
    "                        epochs=EPOCHS,initial_epoch = 0 ,callbacks = [model_checkpoint])\n",
    "\n",
    "\n",
    "\n",
    "# make predictions on the testing/validation images\n",
    "predIdxs = model.predict_generator(test_gen,steps=(NUM_TEST_IMAGES // BATCH) + 1) #should return a 3D tensor with (samples, bounding_boxes , bounding_box offsets)\n",
    "\n",
    "\n",
    "\n",
    "# show a nicely formatted classification report\n",
    "print(\"[INFO] evaluating network...\")\n",
    "print(classification_report(testLabels.argmax(axis=1), predIdxs,target_names=lb.classes_))\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "plot_performance(H ,EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the number of MACs operations of your model (number of flops)\n",
    "flops = tf.profiler.profile(graph,\\\n",
    "     options=tf.profiler.ProfileOptionBuilder.float_operation())\n",
    "print('FLOP = ', flops.total_float_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model found in config file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-87731857ec03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'face.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m416\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_config'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel_config\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No model found in config file.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[0mmodel_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No model found in config file."
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "from keras.models import load_model\n",
    "model1 = load_model('model.h5')\n",
    "image = cv2.imread('face.jpg')\n",
    "image = cv2.resize(image, (416,416))\n",
    "model1.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 711/711 [00:00<00:00, 5481.29it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2134/2134 [00:00<00:00, 4059.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "'''Make the training and testing csv file'''\n",
    "train_txt = \"dataset/fddb/FDDB-folds/annotations_darknet/train.txt\"\n",
    "test_txt = \"dataset/fddb/FDDB-folds/annotations_darknet/test.txt\"\n",
    "path = \"D:/Desktop/Facial-Recognition/Face-Detection/\" #path to Face Detection folder\n",
    "\n",
    "make_csv(path,train_txt, \"training.csv\") #this creates a training.csv file with everything needed for training\n",
    "make_csv(path , test_txt, \"testing.csv\")  #this created the testing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert the yolo output format to the absolute coordinates and place them in the train text file'''\n",
    "text_file = \"train_faces.txt\"\n",
    "training_csv = pd.read_csv(\"training.csv\")\n",
    "f = open(text_file,'a') # a stands for append and w for write\n",
    "\n",
    "for i in range (1, training_csv.shape[0]):\n",
    "    '''For every row of the csv file extract the image file path'''\n",
    "    path = training_csv.iat[i,0]\n",
    "    labels = training_csv.iat[i,1]\n",
    "    inputs = labels.split('!')\n",
    "    image = cv2.imread(path)\n",
    "    image_width = image.shape[1]\n",
    "    image_height = image.shape[0]\n",
    "    inputs.remove(inputs[0])\n",
    "    f.write(path)\n",
    "    for input_ in inputs:\n",
    "        '''For every box in the specific image extract the box offsets\n",
    "        and calculate abs coordinates'''\n",
    "        string =str(input_)\n",
    "        bb= string.split()\n",
    "        input_x = float(bb[1])\n",
    "        input_y = float(bb[2])\n",
    "        input_w =float(bb[3])\n",
    "        input_h =float(bb[4])\n",
    "        class_ = int(bb[0])\n",
    "        absx_center = float(input_x *image_width)\n",
    "        absy_center = float(input_y *image_height)\n",
    "\n",
    "        abs_boxh = float(input_h *image_height)\n",
    "        abs_boxw = float(input_w *image_width)\n",
    "\n",
    "        xmin= int(absx_center - abs_boxw//2)\n",
    "        xmax= int(absx_center + abs_boxw/2)\n",
    "        ymin = int(absy_center - abs_boxh/2)\n",
    "        ymax =int(absy_center + abs_boxh/2)\n",
    "\n",
    "        ''' Check if the conversion was completed succesfully\n",
    "        cv2.rectangle(image, (xmin,ymin),(xmax,ymax), (0,255,255), 3)\n",
    "        cv2.imshow('hello' , image)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "        '''\n",
    "        #add xmin,xmax,ymin,ymax in the file--> image_path box1 box2 ... boxN\n",
    "        box = ','.join([str(x) for x in [xmin,ymin,xmax,ymax,class_]])\n",
    "        f.write(' '+box)\n",
    "    f.write('\\n') #change line after you have written all boxes\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Tiny YOLOv3 model with 6 anchors and 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py:3462: UserWarning: Skipping loading of weights for layer conv2d_10 due to mismatch in shape ((1, 1, 512, 21) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py:3462: UserWarning: Skipping loading of weights for layer conv2d_10 due to mismatch in shape ((21,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py:3462: UserWarning: Skipping loading of weights for layer conv2d_13 due to mismatch in shape ((1, 1, 256, 21) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py:3462: UserWarning: Skipping loading of weights for layer conv2d_13 due to mismatch in shape ((21,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights yolo3/model_data/tiny_yolo_weights.h5.\n",
      "Train on 5620 samples, val on 624 samples, with batch size 1.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n\t [[loss/add_12/_879]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node conv2d_1/convolution:\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\t\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\n\nInput Source operations connected to node conv2d_1/convolution:\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\t\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\n\nOriginal stack trace for 'conv2d_1/convolution':\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 163, in <module>\n    _main()\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 37, in _main\n    freeze_body=0, weights_path='yolo3/model_data/tiny_yolo_weights.h5')\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 119, in create_tiny_model\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\model.py\", line 100, in tiny_yolo_body\n    DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  [Previous line repeated 7 more times]\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 894, in convolution\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 971, in convolution_internal\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n\t [[loss/add_12/_879]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[{{node conv2d_1/convolution}}]]\n0 successful operations.\n0 derived errors ignored.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4c8fd3a69dcb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m     \u001b[0m_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-4c8fd3a69dcb>\u001b[0m in \u001b[0;36m_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_val\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                 initial_epoch=0, callbacks = [checkpoint])\n\u001b[0m\u001b[0;32m     67\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'trained_weights_stage_1.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2222\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2223\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2478\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2479\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnknownError\u001b[0m: 2 root error(s) found.\n  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n\t [[loss/add_12/_879]]\n  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.\n\t [[node conv2d_1/convolution (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3335) ]]\n0 successful operations.\n0 derived errors ignored.\n\nErrors may have originated from an input operation.\nInput Source operations connected to node conv2d_1/convolution:\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\t\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\n\nInput Source operations connected to node conv2d_1/convolution:\n conv2d_1/kernel/read (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:395)\t\n input_1 (defined at D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:507)\n\nOriginal stack trace for 'conv2d_1/convolution':\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 442, in run_forever\n    self._run_once()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\base_events.py\", line 1462, in _run_once\n    handle._run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in execute_request\n    user_expressions, allow_stdin,\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2858, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3063, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 163, in <module>\n    _main()\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 37, in _main\n    freeze_body=0, weights_path='yolo3/model_data/tiny_yolo_weights.h5')\n  File \"<ipython-input-16-4c8fd3a69dcb>\", line 119, in create_tiny_model\n    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\model.py\", line 100, in tiny_yolo_body\n    DarknetConv2D_BN_Leaky(256, (3,3)))(inputs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  File \"D:\\Desktop\\Facial-Recognition\\Face-Detection\\yolo3\\utils.py\", line 16, in <lambda>\n    return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n  [Previous line repeated 7 more times]\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\engine\\topology.py\", line 619, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\layers\\convolutional.py\", line 168, in call\n    dilation_rate=self.dilation_rate)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\", line 3335, in conv2d\n    data_format=tf_data_format)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 894, in convolution\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 971, in convolution_internal\n    name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1071, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"D:\\Conda Installation\\envs\\gpu_python3.6\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "'Import all librares needed to run the train.py file'\n",
    "#import all necessary libraries\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2\n",
    "\"\"\"\n",
    "Retrain the YOLO model for your own dataset.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _main():\n",
    "    annotation_path = 'train_faces.txt'\n",
    "    log_dir = './'\n",
    "    classes_path = 'yolo3/model_data/faces.txt'\n",
    "    anchors_path = 'yolo3/model_data/tiny_yolo_anchors.txt'\n",
    "    class_names = get_classes(classes_path)\n",
    "    num_classes = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "    input_shape = (416,416) # multiple of 32, hw\n",
    "    is_tiny_version = len(anchors)==6 # default setting\n",
    "    if is_tiny_version:\n",
    "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=0, weights_path='yolo3/model_data/tiny_yolo_weights.h5')\n",
    "    #logging = TensorBoard(log_dir=log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "    val_split = 0.1\n",
    "    with open(annotation_path) as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    if True:\n",
    "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "        batch_size = 1\n",
    "        model.save('tiny_yolo.h5')\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=10,\n",
    "                initial_epoch=0, callbacks = [checkpoint])\n",
    "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "    if True:\n",
    "        for i in range(len(model.layers)):\n",
    "            model.layers[i].trainable = True\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "        print('Unfreeze all of the layers.')\n",
    "\n",
    "        batch_size = 12 # note that more GPU memory is required after unfreezing the body\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=50,\n",
    "            initial_epoch=0,\n",
    "            callbacks=[ checkpoint, reduce_lr, early_stopping])\n",
    "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
    "\n",
    "    # Further training if needed.\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu3.6",
   "language": "python",
   "name": "gpu_python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
